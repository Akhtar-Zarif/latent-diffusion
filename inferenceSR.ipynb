{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/CompVis/latent-diffusion.git\n",
    "# !git clone https://github.com/CompVis/taming-transformers\n",
    "# !pip install -e ./taming-transformers\n",
    "# !pip install ipywidgets omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\".\")\n",
    "# sys.path.append('./taming-transformers')\n",
    "# from taming.models import vqgan # checking correct import from taming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bc949a9ce04204b5101d5b90a8c53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(description='Task:', options=('superresolution',), value='superresolution')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %cd latent-diffusion\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "mode = widgets.Select(options=['superresolution'],\n",
    "    value='superresolution', description='Task:')\n",
    "display(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64af38f679914682ba40818ce52c7d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(description='SR resolutions:', options=('64-256', '32-64'), value='64-256')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resolution = widgets.Select(options=['64-256', '32-64'],\n",
    "    value='64-256', description='SR resolutions:')\n",
    "display(resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64-256 selected\n"
     ]
    }
   ],
   "source": [
    "# Define variables\n",
    "\n",
    "## LDM 64->256\n",
    "if resolution.value == \"64-256\":\n",
    "    print(\"64-256 selected\")\n",
    "    # path_conf = \".\\\\models\\\\trained_model_config\\\\ffhq256_sr.yaml\"\n",
    "    # path_ckpt = \".\\\\trained_models\\\\epoch=000052.ckpt\"\n",
    "    # path_ckpt = \".\\\\trained_models\\\\epoch=000049.ckpt\"\n",
    "    path_conf = \".\\\\models\\\\ldm\\\\bsr_sr\\\\config.yaml\"\n",
    "    path_ckpt = \".\\\\models\\\\ldm\\\\bsr_sr\\\\model.ckpt\"\n",
    "\n",
    "\n",
    "    up_f = 4\n",
    "## LDM 32->64\n",
    "elif resolution.value == \"32-64\":\n",
    "    print(\"32-64 selected\")\n",
    "    path_conf = \".\\\\models\\\\trained_model_config\\\\ffhq32-64_sr.yaml\"\n",
    "    path_ckpt = \".\\\\trained_models\\\\epoch=000014.ckpt\"\n",
    "    up_f = 2\n",
    "# path_conf = \".\\\\models\\\\ldm\\\\ffhq256\\\\ffhq256_sr.yaml\"\n",
    "# path_ckpt = \".\\\\trained_models\\\\epoch=000052.ckpt\"\n",
    "\n",
    "# path_conf = \".\\\\models\\\\ldm\\\\ffhq256\\\\ffhq32-64_sr.yaml\"\n",
    "# path_ckpt = \".\\\\trained_models\\\\epoch=000014.ckpt\"\n",
    "# uploaded_img = \"\" # 给一张照片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.path.isfile(path_ckpt)\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from .\\models\\ldm\\bsr_sr\\model.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 113.62 M params.\n",
      "Keeping EMAs of 308.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Restored from ./models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys\n"
     ]
    }
   ],
   "source": [
    "from notebook_helpers import get_local_model\n",
    "model = get_local_model(path_conf, path_ckpt) # load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from notebook_helpers import get_custom_cond\n",
    "\n",
    "# get_custom_cond(mode.value, uploaded_img=uploaded_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2314ded5e7084e1e95dce3c2eeda3816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Select conditioning:', index=5, options=('face256_1.png', 'face32_1.png', 'face32_2.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from notebook_helpers import get_cond_options, get_cond\n",
    "dir, options = get_cond_options(mode.value)\n",
    "cond_choice = widgets.RadioButtons(\n",
    "        options=options,\n",
    "        description='Select conditioning:',\n",
    "        disabled=False,\n",
    "        value = \"face64_1.png\"\n",
    "    )\n",
    "display(cond_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heyeheyhey\n",
      "original c shape: torch.Size([1, 3, 64, 64])\n",
      "heyeheyhey\n",
      "torch.Size([1, 64, 64, 3]) torch.Size([1, 256, 256, 3])\n",
      "reducing Kernel\n",
      "Plotting: Switched to EMA weights\n",
      "Sampling with eta = 1.0; steps: 200\n",
      "Data shape for DDIM sampling is (1, 3, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting: Restored training weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 2D or 3D (batch mode) tensor for input with possibly 0 batch size and non-zero dimensions for input, but got: [1, 16384, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m custom_steps \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m\n\u001b[0;32m      4\u001b[0m cond_choice_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mdir\u001b[39m, cond_choice\u001b[39m.\u001b[39mvalue)\n\u001b[1;32m----> 5\u001b[0m logs \u001b[39m=\u001b[39m run(model[\u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m], cond_choice_path, mode\u001b[39m.\u001b[39;49mvalue, custom_steps, up_f)\n",
      "File \u001b[1;32mc:\\Users\\cj020\\Desktop\\Diffusion Research\\latent-diffusion\\notebook_helpers.py:188\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(model, selected_path, task, custom_steps, up_f, resize_enabled, classifier_ckpt, global_step)\u001b[0m\n\u001b[0;32m    185\u001b[0m         x_T \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, custom_shape[\u001b[39m1\u001b[39m], custom_shape[\u001b[39m2\u001b[39m], custom_shape[\u001b[39m3\u001b[39m])\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    186\u001b[0m         x_T \u001b[39m=\u001b[39m repeat(x_T, \u001b[39m'\u001b[39m\u001b[39m1 c h w -> b c h w\u001b[39m\u001b[39m'\u001b[39m, b\u001b[39m=\u001b[39mcustom_shape[\u001b[39m0\u001b[39m])\n\u001b[1;32m--> 188\u001b[0m     logs \u001b[39m=\u001b[39m make_convolutional_sample(example, model,\n\u001b[0;32m    189\u001b[0m                                      mode\u001b[39m=\u001b[39;49mmode, custom_steps\u001b[39m=\u001b[39;49mcustom_steps,\n\u001b[0;32m    190\u001b[0m                                      eta\u001b[39m=\u001b[39;49meta, swap_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m , masked\u001b[39m=\u001b[39;49mmasked,\n\u001b[0;32m    191\u001b[0m                                      invert_mask\u001b[39m=\u001b[39;49minvert_mask, quantize_x0\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    192\u001b[0m                                      custom_schedule\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, decode_interval\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m    193\u001b[0m                                      resize_enabled\u001b[39m=\u001b[39;49mresize_enabled, custom_shape\u001b[39m=\u001b[39;49mcustom_shape,\n\u001b[0;32m    194\u001b[0m                                      temperature\u001b[39m=\u001b[39;49mtemperature, noise_dropout\u001b[39m=\u001b[39;49m\u001b[39m0.\u001b[39;49m,\n\u001b[0;32m    195\u001b[0m                                      corrector\u001b[39m=\u001b[39;49mguider, corrector_kwargs\u001b[39m=\u001b[39;49mckwargs, x_T\u001b[39m=\u001b[39;49mx_T, save_intermediate_vid\u001b[39m=\u001b[39;49msave_intermediate_vid,\n\u001b[0;32m    196\u001b[0m                                      make_progrow\u001b[39m=\u001b[39;49mmake_progrow,ddim_use_x0_pred\u001b[39m=\u001b[39;49mddim_use_x0_pred\n\u001b[0;32m    197\u001b[0m                                      )\n\u001b[0;32m    198\u001b[0m \u001b[39mreturn\u001b[39;00m logs\n",
      "File \u001b[1;32mc:\\Users\\cj020\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cj020\\Desktop\\Diffusion Research\\latent-diffusion\\notebook_helpers.py:261\u001b[0m, in \u001b[0;36mmake_convolutional_sample\u001b[1;34m(batch, model, mode, custom_steps, eta, swap_mode, masked, invert_mask, quantize_x0, custom_schedule, decode_interval, resize_enabled, custom_shape, temperature, noise_dropout, corrector, corrector_kwargs, x_T, save_intermediate_vid, make_progrow, ddim_use_x0_pred)\u001b[0m\n\u001b[0;32m    258\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    259\u001b[0m img_cb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 261\u001b[0m sample, intermediates \u001b[39m=\u001b[39m convsample_ddim(model, c, steps\u001b[39m=\u001b[39;49mcustom_steps, shape\u001b[39m=\u001b[39;49mz\u001b[39m.\u001b[39;49mshape,\n\u001b[0;32m    262\u001b[0m                                         eta\u001b[39m=\u001b[39;49meta,\n\u001b[0;32m    263\u001b[0m                                         quantize_x0\u001b[39m=\u001b[39;49mquantize_x0, img_callback\u001b[39m=\u001b[39;49mimg_cb, mask\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, x0\u001b[39m=\u001b[39;49mz0,\n\u001b[0;32m    264\u001b[0m                                         temperature\u001b[39m=\u001b[39;49mtemperature, noise_dropout\u001b[39m=\u001b[39;49mnoise_dropout,\n\u001b[0;32m    265\u001b[0m                                         score_corrector\u001b[39m=\u001b[39;49mcorrector, corrector_kwargs\u001b[39m=\u001b[39;49mcorrector_kwargs,\n\u001b[0;32m    266\u001b[0m                                         x_T\u001b[39m=\u001b[39;49mx_T, log_every_t\u001b[39m=\u001b[39;49mlog_every_t)\n\u001b[0;32m    267\u001b[0m t1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    269\u001b[0m \u001b[39mif\u001b[39;00m ddim_use_x0_pred:\n",
      "File \u001b[1;32mc:\\Users\\cj020\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cj020\\Desktop\\Diffusion Research\\latent-diffusion\\notebook_helpers.py:212\u001b[0m, in \u001b[0;36mconvsample_ddim\u001b[1;34m(model, cond, steps, shape, eta, callback, normals_sequence, mask, x0, quantize_x0, img_callback, temperature, noise_dropout, score_corrector, corrector_kwargs, x_T, log_every_t)\u001b[0m\n\u001b[0;32m    210\u001b[0m shape \u001b[39m=\u001b[39m shape[\u001b[39m1\u001b[39m:]  \u001b[39m# cut batch dim\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSampling with eta = \u001b[39m\u001b[39m{\u001b[39;00meta\u001b[39m}\u001b[39;00m\u001b[39m; steps: \u001b[39m\u001b[39m{\u001b[39;00msteps\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 212\u001b[0m samples, intermediates \u001b[39m=\u001b[39m ddim\u001b[39m.\u001b[39;49msample(steps, batch_size\u001b[39m=\u001b[39;49mbs, shape\u001b[39m=\u001b[39;49mshape, conditioning\u001b[39m=\u001b[39;49mcond, callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    213\u001b[0m                                      normals_sequence\u001b[39m=\u001b[39;49mnormals_sequence, quantize_x0\u001b[39m=\u001b[39;49mquantize_x0, eta\u001b[39m=\u001b[39;49meta,\n\u001b[0;32m    214\u001b[0m                                      mask\u001b[39m=\u001b[39;49mmask, x0\u001b[39m=\u001b[39;49mx0, temperature\u001b[39m=\u001b[39;49mtemperature, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    215\u001b[0m                                      score_corrector\u001b[39m=\u001b[39;49mscore_corrector,\n\u001b[0;32m    216\u001b[0m                                      corrector_kwargs\u001b[39m=\u001b[39;49mcorrector_kwargs, x_T\u001b[39m=\u001b[39;49mx_T)\n\u001b[0;32m    218\u001b[0m \u001b[39mreturn\u001b[39;00m samples, intermediates\n",
      "File \u001b[1;32mc:\\Users\\cj020\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cj020\\Desktop\\Diffusion Research\\latent-diffusion\\ldm\\models\\diffusion\\ddim.py:95\u001b[0m, in \u001b[0;36mDDIMSampler.sample\u001b[1;34m(self, S, batch_size, shape, conditioning, callback, normals_sequence, img_callback, quantize_x0, eta, mask, x0, temperature, noise_dropout, score_corrector, corrector_kwargs, verbose, x_T, log_every_t, unconditional_guidance_scale, unconditional_conditioning, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m size \u001b[39m=\u001b[39m (batch_size, C, H, W)\n\u001b[0;32m     93\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mData shape for DDIM sampling is \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m, eta \u001b[39m\u001b[39m{\u001b[39;00meta\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 95\u001b[0m samples, intermediates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mddim_sampling(conditioning, size,\n\u001b[0;32m     96\u001b[0m                                             callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m     97\u001b[0m                                             img_callback\u001b[39m=\u001b[39;49mimg_callback,\n\u001b[0;32m     98\u001b[0m                                             quantize_denoised\u001b[39m=\u001b[39;49mquantize_x0,\n\u001b[0;32m     99\u001b[0m                                             mask\u001b[39m=\u001b[39;49mmask, x0\u001b[39m=\u001b[39;49mx0,\n\u001b[0;32m    100\u001b[0m                                             ddim_use_original_steps\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    101\u001b[0m                                             noise_dropout\u001b[39m=\u001b[39;49mnoise_dropout,\n\u001b[0;32m    102\u001b[0m                                             temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[0;32m    103\u001b[0m                                             score_corrector\u001b[39m=\u001b[39;49mscore_corrector,\n\u001b[0;32m    104\u001b[0m                                             corrector_kwargs\u001b[39m=\u001b[39;49mcorrector_kwargs,\n\u001b[0;32m    105\u001b[0m                                             x_T\u001b[39m=\u001b[39;49mx_T,\n\u001b[0;32m    106\u001b[0m                                             log_every_t\u001b[39m=\u001b[39;49mlog_every_t,\n\u001b[0;32m    107\u001b[0m                                             unconditional_guidance_scale\u001b[39m=\u001b[39;49munconditional_guidance_scale,\n\u001b[0;32m    108\u001b[0m                                             unconditional_conditioning\u001b[39m=\u001b[39;49munconditional_conditioning,\n\u001b[0;32m    109\u001b[0m                                             )\n\u001b[0;32m    110\u001b[0m \u001b[39mreturn\u001b[39;00m samples, intermediates\n",
      "File \u001b[1;32mc:\\Users\\cj020\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cj020\\Desktop\\Diffusion Research\\latent-diffusion\\ldm\\models\\diffusion\\ddim.py:148\u001b[0m, in \u001b[0;36mDDIMSampler.ddim_sampling\u001b[1;34m(self, cond, shape, x_T, ddim_use_original_steps, callback, timesteps, quantize_denoised, mask, x0, img_callback, log_every_t, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning)\u001b[0m\n\u001b[0;32m    145\u001b[0m     img_orig \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mq_sample(x0, ts)  \u001b[39m# TODO: deterministic forward pass?\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     img \u001b[39m=\u001b[39m img_orig \u001b[39m*\u001b[39m mask \u001b[39m+\u001b[39m (\u001b[39m1.\u001b[39m \u001b[39m-\u001b[39m mask) \u001b[39m*\u001b[39m img\n\u001b[1;32m--> 148\u001b[0m outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_sample_ddim(img, cond, ts, index\u001b[39m=\u001b[39;49mindex, use_original_steps\u001b[39m=\u001b[39;49mddim_use_original_steps,\n\u001b[0;32m    149\u001b[0m                           quantize_denoised\u001b[39m=\u001b[39;49mquantize_denoised, temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[0;32m    150\u001b[0m                           noise_dropout\u001b[39m=\u001b[39;49mnoise_dropout, score_corrector\u001b[39m=\u001b[39;49mscore_corrector,\n\u001b[0;32m    151\u001b[0m                           corrector_kwargs\u001b[39m=\u001b[39;49mcorrector_kwargs,\n\u001b[0;32m    152\u001b[0m                           unconditional_guidance_scale\u001b[39m=\u001b[39;49munconditional_guidance_scale,\n\u001b[0;32m    153\u001b[0m                           unconditional_conditioning\u001b[39m=\u001b[39;49munconditional_conditioning)\n\u001b[0;32m    154\u001b[0m img, pred_x0 \u001b[39m=\u001b[39m outs\n\u001b[0;32m    155\u001b[0m \u001b[39mif\u001b[39;00m callback: callback(i)\n",
      "File \u001b[1;32mc:\\Users\\cj020\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cj020\\Desktop\\Diffusion Research\\latent-diffusion\\ldm\\models\\diffusion\\ddim.py:171\u001b[0m, in \u001b[0;36mDDIMSampler.p_sample_ddim\u001b[1;34m(self, x, c, t, index, repeat_noise, use_original_steps, quantize_denoised, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning)\u001b[0m\n\u001b[0;32m    168\u001b[0m b, \u001b[39m*\u001b[39m_, device \u001b[39m=\u001b[39m \u001b[39m*\u001b[39mx\u001b[39m.\u001b[39mshape, x\u001b[39m.\u001b[39mdevice\n\u001b[0;32m    170\u001b[0m \u001b[39mif\u001b[39;00m unconditional_conditioning \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m unconditional_guidance_scale \u001b[39m==\u001b[39m \u001b[39m1.\u001b[39m:\n\u001b[1;32m--> 171\u001b[0m     e_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mapply_model(x, t, c)\n\u001b[0;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     x_in \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x] \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cj020\\Desktop\\Diffusion Research\\latent-diffusion\\ldm\\models\\diffusion\\ddpm.py:937\u001b[0m, in \u001b[0;36mLatentDiffusion.apply_model\u001b[1;34m(self, x_noisy, t, cond, return_ids)\u001b[0m\n\u001b[0;32m    933\u001b[0m stride \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_input_params[\u001b[39m\"\u001b[39m\u001b[39mstride\u001b[39m\u001b[39m\"\u001b[39m]  \u001b[39m# eg. (64, 64)\u001b[39;00m\n\u001b[0;32m    935\u001b[0m h, w \u001b[39m=\u001b[39m x_noisy\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]\n\u001b[1;32m--> 937\u001b[0m fold, unfold, normalization, weighting \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_fold_unfold(x_noisy, ks, stride)\n\u001b[0;32m    939\u001b[0m z \u001b[39m=\u001b[39m unfold(x_noisy)  \u001b[39m# (bn, nc * prod(**ks), L)\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[39m# Reshape to img shape\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cj020\\Desktop\\Diffusion Research\\latent-diffusion\\ldm\\models\\diffusion\\ddpm.py:621\u001b[0m, in \u001b[0;36mLatentDiffusion.get_fold_unfold\u001b[1;34m(self, x, kernel_size, stride, uf, df)\u001b[0m\n\u001b[0;32m    618\u001b[0m     fold \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mFold(output_size\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfold_params)\n\u001b[0;32m    620\u001b[0m     weighting \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_weighting(kernel_size[\u001b[39m0\u001b[39m], kernel_size[\u001b[39m1\u001b[39m], Ly, Lx, x\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m--> 621\u001b[0m     normalization \u001b[39m=\u001b[39m fold(weighting)\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, h, w)  \u001b[39m# normalizes the overlap\u001b[39;00m\n\u001b[0;32m    622\u001b[0m     weighting \u001b[39m=\u001b[39m weighting\u001b[39m.\u001b[39mview((\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, kernel_size[\u001b[39m0\u001b[39m], kernel_size[\u001b[39m1\u001b[39m], Ly \u001b[39m*\u001b[39m Lx))\n\u001b[0;32m    624\u001b[0m \u001b[39melif\u001b[39;00m uf \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m df \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\cj020\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\cj020\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\nn\\modules\\fold.py:146\u001b[0m, in \u001b[0;36mFold.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 146\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mfold(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation,\n\u001b[0;32m    147\u001b[0m                   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride)\n",
      "File \u001b[1;32mc:\\Users\\cj020\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\nn\\functional.py:4691\u001b[0m, in \u001b[0;36mfold\u001b[1;34m(input, output_size, kernel_size, dilation, padding, stride)\u001b[0m\n\u001b[0;32m   4687\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n\u001b[0;32m   4688\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   4689\u001b[0m         fold, (\u001b[39minput\u001b[39m,), \u001b[39minput\u001b[39m, output_size, kernel_size, dilation\u001b[39m=\u001b[39mdilation, padding\u001b[39m=\u001b[39mpadding, stride\u001b[39m=\u001b[39mstride\n\u001b[0;32m   4690\u001b[0m     )\n\u001b[1;32m-> 4691\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcol2im(\n\u001b[0;32m   4692\u001b[0m     \u001b[39minput\u001b[39;49m, _pair(output_size), _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride)\n\u001b[0;32m   4693\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 2D or 3D (batch mode) tensor for input with possibly 0 batch size and non-zero dimensions for input, but got: [1, 16384, 0]"
     ]
    }
   ],
   "source": [
    "from notebook_helpers import run\n",
    "import os\n",
    "custom_steps = 200\n",
    "cond_choice_path = os.path.join(dir, cond_choice.value)\n",
    "logs = run(model[\"model\"], cond_choice_path, mode.value, custom_steps, up_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import IPython.display as d\n",
    "from PIL import Image\n",
    "\n",
    "sample = logs[\"sample\"]\n",
    "sample = sample.detach().cpu()\n",
    "sample = torch.clamp(sample, -1., 1.)\n",
    "sample = (sample + 1.) / 2. * 255\n",
    "sample = sample.numpy().astype(np.uint8)\n",
    "sample = np.transpose(sample, (0, 2, 3, 1))\n",
    "print(sample.shape)\n",
    "a = Image.fromarray(sample[0])\n",
    "display(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b11eaf0e94fc111c724b4ee42f36a114f13a982891d5c8efe533c6716daa5c0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

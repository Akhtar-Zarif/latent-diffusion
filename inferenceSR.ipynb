{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/CompVis/latent-diffusion.git\n",
    "# !git clone https://github.com/CompVis/taming-transformers\n",
    "# !pip install -e ./taming-transformers\n",
    "# !pip install ipywidgets omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\".\")\n",
    "# sys.path.append('./taming-transformers')\n",
    "# from taming.models import vqgan # checking correct import from taming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10f26211f3147e682af46f7a52f2b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(description='Task:', options=('superresolution',), value='superresolution')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %cd latent-diffusion\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "mode = widgets.Select(options=['superresolution'],\n",
    "    value='superresolution', description='Task:')\n",
    "display(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a975522ac9417983a5399574614baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(description='SR resolutions:', index=1, options=('64-256', '32-64'), value='32-64')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resolution = widgets.Select(options=['64-256', '32-64'],\n",
    "    value='32-64', description='SR resolutions:')\n",
    "display(resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64-256 selected\n"
     ]
    }
   ],
   "source": [
    "# Define variables\n",
    "\n",
    "## LDM 64->256\n",
    "if resolution.value == \"64-256\":\n",
    "    print(\"64-256 selected\")\n",
    "    path_conf = \".\\\\models\\\\ldm\\\\ffhq256\\\\ffhq256_sr.yaml\"\n",
    "    path_ckpt = \".\\\\trained_models\\\\epoch=000052.ckpt\"\n",
    "    up_f = 4\n",
    "## LDM 32->64\n",
    "elif resolution.value == \"32-64\":\n",
    "    print(\"32-64 selected\")\n",
    "    path_conf = \".\\\\models\\\\ldm\\\\ffhq256\\\\ffhq32-64_sr.yaml\"\n",
    "    path_ckpt = \".\\\\trained_models\\\\epoch=000014.ckpt\"\n",
    "    up_f = 2\n",
    "# path_conf = \".\\\\models\\\\ldm\\\\ffhq256\\\\ffhq256_sr.yaml\"\n",
    "# path_ckpt = \".\\\\trained_models\\\\epoch=000052.ckpt\"\n",
    "\n",
    "# path_conf = \".\\\\models\\\\ldm\\\\ffhq256\\\\ffhq32-64_sr.yaml\"\n",
    "# path_ckpt = \".\\\\trained_models\\\\epoch=000014.ckpt\"\n",
    "# uploaded_img = \"\" # 给一张照片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.path.isfile(path_ckpt)\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from .\\trained_models\\epoch=000052.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 113.62 M params.\n",
      "Keeping EMAs of 308.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Restored from ./models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys\n"
     ]
    }
   ],
   "source": [
    "from notebook_helpers import get_local_model\n",
    "model = get_local_model(path_conf, path_ckpt) # load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from notebook_helpers import get_custom_cond\n",
    "\n",
    "# get_custom_cond(mode.value, uploaded_img=uploaded_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd68a2462bd0435485d136e252e50c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Select conditioning:', options=('face256_1.png', 'face32_1.png', 'face32_2.png', 'fa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from notebook_helpers import get_cond_options, get_cond\n",
    "dir, options = get_cond_options(mode.value)\n",
    "cond_choice = widgets.RadioButtons(\n",
    "        options=options,\n",
    "        description='Select conditioning:',\n",
    "        disabled=False\n",
    "    )\n",
    "display(cond_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heyeheyhey\n",
      "original c shape: torch.Size([1, 3, 64, 64])\n",
      "heyeheyhey\n",
      "torch.Size([1, 64, 64, 3]) torch.Size([1, 256, 256, 3])\n",
      "reducing Kernel\n",
      "Plotting: Switched to EMA weights\n",
      "Sampling with eta = 1.0; steps: 200\n",
      "Data shape for DDIM sampling is (1, 3, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting: Restored training weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 2D or 3D (batch mode) tensor for input with possibly 0 batch size and non-zero dimensions for input, but got: [1, 16384, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m custom_steps \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m\n\u001b[0;32m      4\u001b[0m cond_choice_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mdir\u001b[39m, cond_choice\u001b[39m.\u001b[39mvalue)\n\u001b[1;32m----> 5\u001b[0m logs \u001b[39m=\u001b[39m run(model[\u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m], cond_choice_path, mode\u001b[39m.\u001b[39;49mvalue, custom_steps, up_f)\n",
      "File \u001b[1;32mc:\\Users\\cj020\\Desktop\\Diffusion Research\\latent-diffusion\\notebook_helpers.py:188\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(model, selected_path, task, custom_steps, up_f, resize_enabled, classifier_ckpt, global_step)\u001b[0m\n\u001b[0;32m    185\u001b[0m         x_T \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, custom_shape[\u001b[39m1\u001b[39m], custom_shape[\u001b[39m2\u001b[39m], custom_shape[\u001b[39m3\u001b[39m])\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    186\u001b[0m         x_T \u001b[39m=\u001b[39m repeat(x_T, \u001b[39m'\u001b[39m\u001b[39m1 c h w -> b c h w\u001b[39m\u001b[39m'\u001b[39m, b\u001b[39m=\u001b[39mcustom_shape[\u001b[39m0\u001b[39m])\n\u001b[1;32m--> 188\u001b[0m     logs \u001b[39m=\u001b[39m make_convolutional_sample(example, model,\n\u001b[0;32m    189\u001b[0m                                      mode\u001b[39m=\u001b[39;49mmode, custom_steps\u001b[39m=\u001b[39;49mcustom_steps,\n\u001b[0;32m    190\u001b[0m                                      eta\u001b[39m=\u001b[39;49meta, swap_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m , masked\u001b[39m=\u001b[39;49mmasked,\n\u001b[0;32m    191\u001b[0m                                      invert_mask\u001b[39m=\u001b[39;49minvert_mask, quantize_x0\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    192\u001b[0m                                      custom_schedule\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, decode_interval\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m    193\u001b[0m                                      resize_enabled\u001b[39m=\u001b[39;49mresize_enabled, custom_shape\u001b[39m=\u001b[39;49mcustom_shape,\n\u001b[0;32m    194\u001b[0m                                      temperature\u001b[39m=\u001b[39;49mtemperature, noise_dropout\u001b[39m=\u001b[39;49m\u001b[39m0.\u001b[39;49m,\n\u001b[0;32m    195\u001b[0m                                      corrector\u001b[39m=\u001b[39;49mguider, corrector_kwargs\u001b[39m=\u001b[39;49mckwargs, x_T\u001b[39m=\u001b[39;49mx_T, save_intermediate_vid\u001b[39m=\u001b[39;49msave_intermediate_vid,\n\u001b[0;32m    196\u001b[0m                                      make_progrow\u001b[39m=\u001b[39;49mmake_progrow,ddim_use_x0_pred\u001b[39m=\u001b[39;49mddim_use_x0_pred\n\u001b[0;32m    197\u001b[0m                                      )\n\u001b[0;32m    198\u001b[0m \u001b[39mreturn\u001b[39;00m logs\n",
      "File \u001b[1;32mc:\\Users\\cj020\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cj020\\Desktop\\Diffusion Research\\latent-diffusion\\notebook_helpers.py:261\u001b[0m, in \u001b[0;36mmake_convolutional_sample\u001b[1;34m(batch, model, mode, custom_steps, eta, swap_mode, masked, invert_mask, quantize_x0, custom_schedule, decode_interval, resize_enabled, custom_shape, temperature, noise_dropout, corrector, corrector_kwargs, x_T, save_intermediate_vid, make_progrow, ddim_use_x0_pred)\u001b[0m\n\u001b[0;32m    258\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    259\u001b[0m img_cb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 261\u001b[0m sample, intermediates \u001b[39m=\u001b[39m convsample_ddim(model, c, steps\u001b[39m=\u001b[39;49mcustom_steps, shape\u001b[39m=\u001b[39;49mz\u001b[39m.\u001b[39;49mshape,\n\u001b[0;32m    262\u001b[0m                                         eta\u001b[39m=\u001b[39;49meta,\n\u001b[0;32m    263\u001b[0m                                         quantize_x0\u001b[39m=\u001b[39;49mquantize_x0, img_callback\u001b[39m=\u001b[39;49mimg_cb, mask\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, x0\u001b[39m=\u001b[39;49mz0,\n\u001b[0;32m    264\u001b[0m                                         temperature\u001b[39m=\u001b[39;49mtemperature, noise_dropout\u001b[39m=\u001b[39;49mnoise_dropout,\n\u001b[0;32m    265\u001b[0m                                         score_corrector\u001b[39m=\u001b[39;49mcorrector, corrector_kwargs\u001b[39m=\u001b[39;49mcorrector_kwargs,\n\u001b[0;32m    266\u001b[0m                                         x_T\u001b[39m=\u001b[39;49mx_T, log_every_t\u001b[39m=\u001b[39;49mlog_every_t)\n\u001b[0;32m    267\u001b[0m t1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    269\u001b[0m \u001b[39mif\u001b[39;00m ddim_use_x0_pred:\n",
      "File \u001b[1;32mc:\\Users\\cj020\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cj020\\Desktop\\Diffusion Research\\latent-diffusion\\notebook_helpers.py:212\u001b[0m, in \u001b[0;36mconvsample_ddim\u001b[1;34m(model, cond, steps, shape, eta, callback, normals_sequence, mask, x0, quantize_x0, img_callback, temperature, noise_dropout, score_corrector, corrector_kwargs, x_T, log_every_t)\u001b[0m\n\u001b[0;32m    210\u001b[0m shape \u001b[39m=\u001b[39m shape[\u001b[39m1\u001b[39m:]  \u001b[39m# cut batch dim\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSampling with eta = \u001b[39m\u001b[39m{\u001b[39;00meta\u001b[39m}\u001b[39;00m\u001b[39m; steps: \u001b[39m\u001b[39m{\u001b[39;00msteps\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 212\u001b[0m samples, intermediates \u001b[39m=\u001b[39m ddim\u001b[39m.\u001b[39;49msample(steps, batch_size\u001b[39m=\u001b[39;49mbs, shape\u001b[39m=\u001b[39;49mshape, conditioning\u001b[39m=\u001b[39;49mcond, callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    213\u001b[0m                                      normals_sequence\u001b[39m=\u001b[39;49mnormals_sequence, quantize_x0\u001b[39m=\u001b[39;49mquantize_x0, eta\u001b[39m=\u001b[39;49meta,\n\u001b[0;32m    214\u001b[0m                                      mask\u001b[39m=\u001b[39;49mmask, x0\u001b[39m=\u001b[39;49mx0, temperature\u001b[39m=\u001b[39;49mtemperature, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    215\u001b[0m                                      score_corrector\u001b[39m=\u001b[39;49mscore_corrector,\n\u001b[0;32m    216\u001b[0m                                      corrector_kwargs\u001b[39m=\u001b[39;49mcorrector_kwargs, x_T\u001b[39m=\u001b[39;49mx_T)\n\u001b[0;32m    218\u001b[0m \u001b[39mreturn\u001b[39;00m samples, intermediates\n",
      "File \u001b[1;32mc:\\Users\\cj020\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cj020\\Desktop\\Diffusion Research\\latent-diffusion\\ldm\\models\\diffusion\\ddim.py:95\u001b[0m, in \u001b[0;36mDDIMSampler.sample\u001b[1;34m(self, S, batch_size, shape, conditioning, callback, normals_sequence, img_callback, quantize_x0, eta, mask, x0, temperature, noise_dropout, score_corrector, corrector_kwargs, verbose, x_T, log_every_t, unconditional_guidance_scale, unconditional_conditioning, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m size \u001b[39m=\u001b[39m (batch_size, C, H, W)\n\u001b[0;32m     93\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mData shape for DDIM sampling is \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m, eta \u001b[39m\u001b[39m{\u001b[39;00meta\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 95\u001b[0m samples, intermediates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mddim_sampling(conditioning, size,\n\u001b[0;32m     96\u001b[0m                                             callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m     97\u001b[0m                                             img_callback\u001b[39m=\u001b[39;49mimg_callback,\n\u001b[0;32m     98\u001b[0m                                             quantize_denoised\u001b[39m=\u001b[39;49mquantize_x0,\n\u001b[0;32m     99\u001b[0m                                             mask\u001b[39m=\u001b[39;49mmask, x0\u001b[39m=\u001b[39;49mx0,\n\u001b[0;32m    100\u001b[0m                                             ddim_use_original_steps\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    101\u001b[0m                                             noise_dropout\u001b[39m=\u001b[39;49mnoise_dropout,\n\u001b[0;32m    102\u001b[0m                                             temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[0;32m    103\u001b[0m                                             score_corrector\u001b[39m=\u001b[39;49mscore_corrector,\n\u001b[0;32m    104\u001b[0m                                             corrector_kwargs\u001b[39m=\u001b[39;49mcorrector_kwargs,\n\u001b[0;32m    105\u001b[0m                                             x_T\u001b[39m=\u001b[39;49mx_T,\n\u001b[0;32m    106\u001b[0m                                             log_every_t\u001b[39m=\u001b[39;49mlog_every_t,\n\u001b[0;32m    107\u001b[0m                                             unconditional_guidance_scale\u001b[39m=\u001b[39;49munconditional_guidance_scale,\n\u001b[0;32m    108\u001b[0m                                             unconditional_conditioning\u001b[39m=\u001b[39;49munconditional_conditioning,\n\u001b[0;32m    109\u001b[0m                                             )\n\u001b[0;32m    110\u001b[0m \u001b[39mreturn\u001b[39;00m samples, intermediates\n",
      "File \u001b[1;32mc:\\Users\\cj020\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cj020\\Desktop\\Diffusion Research\\latent-diffusion\\ldm\\models\\diffusion\\ddim.py:148\u001b[0m, in \u001b[0;36mDDIMSampler.ddim_sampling\u001b[1;34m(self, cond, shape, x_T, ddim_use_original_steps, callback, timesteps, quantize_denoised, mask, x0, img_callback, log_every_t, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning)\u001b[0m\n\u001b[0;32m    145\u001b[0m     img_orig \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mq_sample(x0, ts)  \u001b[39m# TODO: deterministic forward pass?\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     img \u001b[39m=\u001b[39m img_orig \u001b[39m*\u001b[39m mask \u001b[39m+\u001b[39m (\u001b[39m1.\u001b[39m \u001b[39m-\u001b[39m mask) \u001b[39m*\u001b[39m img\n\u001b[1;32m--> 148\u001b[0m outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_sample_ddim(img, cond, ts, index\u001b[39m=\u001b[39;49mindex, use_original_steps\u001b[39m=\u001b[39;49mddim_use_original_steps,\n\u001b[0;32m    149\u001b[0m                           quantize_denoised\u001b[39m=\u001b[39;49mquantize_denoised, temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[0;32m    150\u001b[0m                           noise_dropout\u001b[39m=\u001b[39;49mnoise_dropout, score_corrector\u001b[39m=\u001b[39;49mscore_corrector,\n\u001b[0;32m    151\u001b[0m                           corrector_kwargs\u001b[39m=\u001b[39;49mcorrector_kwargs,\n\u001b[0;32m    152\u001b[0m                           unconditional_guidance_scale\u001b[39m=\u001b[39;49munconditional_guidance_scale,\n\u001b[0;32m    153\u001b[0m                           unconditional_conditioning\u001b[39m=\u001b[39;49munconditional_conditioning)\n\u001b[0;32m    154\u001b[0m img, pred_x0 \u001b[39m=\u001b[39m outs\n\u001b[0;32m    155\u001b[0m \u001b[39mif\u001b[39;00m callback: callback(i)\n",
      "File \u001b[1;32mc:\\Users\\cj020\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cj020\\Desktop\\Diffusion Research\\latent-diffusion\\ldm\\models\\diffusion\\ddim.py:171\u001b[0m, in \u001b[0;36mDDIMSampler.p_sample_ddim\u001b[1;34m(self, x, c, t, index, repeat_noise, use_original_steps, quantize_denoised, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning)\u001b[0m\n\u001b[0;32m    168\u001b[0m b, \u001b[39m*\u001b[39m_, device \u001b[39m=\u001b[39m \u001b[39m*\u001b[39mx\u001b[39m.\u001b[39mshape, x\u001b[39m.\u001b[39mdevice\n\u001b[0;32m    170\u001b[0m \u001b[39mif\u001b[39;00m unconditional_conditioning \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m unconditional_guidance_scale \u001b[39m==\u001b[39m \u001b[39m1.\u001b[39m:\n\u001b[1;32m--> 171\u001b[0m     e_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mapply_model(x, t, c)\n\u001b[0;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     x_in \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x] \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cj020\\Desktop\\Diffusion Research\\latent-diffusion\\ldm\\models\\diffusion\\ddpm.py:937\u001b[0m, in \u001b[0;36mLatentDiffusion.apply_model\u001b[1;34m(self, x_noisy, t, cond, return_ids)\u001b[0m\n\u001b[0;32m    933\u001b[0m stride \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_input_params[\u001b[39m\"\u001b[39m\u001b[39mstride\u001b[39m\u001b[39m\"\u001b[39m]  \u001b[39m# eg. (64, 64)\u001b[39;00m\n\u001b[0;32m    935\u001b[0m h, w \u001b[39m=\u001b[39m x_noisy\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]\n\u001b[1;32m--> 937\u001b[0m fold, unfold, normalization, weighting \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_fold_unfold(x_noisy, ks, stride)\n\u001b[0;32m    939\u001b[0m z \u001b[39m=\u001b[39m unfold(x_noisy)  \u001b[39m# (bn, nc * prod(**ks), L)\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[39m# Reshape to img shape\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cj020\\Desktop\\Diffusion Research\\latent-diffusion\\ldm\\models\\diffusion\\ddpm.py:621\u001b[0m, in \u001b[0;36mLatentDiffusion.get_fold_unfold\u001b[1;34m(self, x, kernel_size, stride, uf, df)\u001b[0m\n\u001b[0;32m    618\u001b[0m     fold \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mFold(output_size\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfold_params)\n\u001b[0;32m    620\u001b[0m     weighting \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_weighting(kernel_size[\u001b[39m0\u001b[39m], kernel_size[\u001b[39m1\u001b[39m], Ly, Lx, x\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m--> 621\u001b[0m     normalization \u001b[39m=\u001b[39m fold(weighting)\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, h, w)  \u001b[39m# normalizes the overlap\u001b[39;00m\n\u001b[0;32m    622\u001b[0m     weighting \u001b[39m=\u001b[39m weighting\u001b[39m.\u001b[39mview((\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, kernel_size[\u001b[39m0\u001b[39m], kernel_size[\u001b[39m1\u001b[39m], Ly \u001b[39m*\u001b[39m Lx))\n\u001b[0;32m    624\u001b[0m \u001b[39melif\u001b[39;00m uf \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m df \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\cj020\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\cj020\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\nn\\modules\\fold.py:146\u001b[0m, in \u001b[0;36mFold.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 146\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mfold(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation,\n\u001b[0;32m    147\u001b[0m                   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride)\n",
      "File \u001b[1;32mc:\\Users\\cj020\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\nn\\functional.py:4691\u001b[0m, in \u001b[0;36mfold\u001b[1;34m(input, output_size, kernel_size, dilation, padding, stride)\u001b[0m\n\u001b[0;32m   4687\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n\u001b[0;32m   4688\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   4689\u001b[0m         fold, (\u001b[39minput\u001b[39m,), \u001b[39minput\u001b[39m, output_size, kernel_size, dilation\u001b[39m=\u001b[39mdilation, padding\u001b[39m=\u001b[39mpadding, stride\u001b[39m=\u001b[39mstride\n\u001b[0;32m   4690\u001b[0m     )\n\u001b[1;32m-> 4691\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcol2im(\n\u001b[0;32m   4692\u001b[0m     \u001b[39minput\u001b[39;49m, _pair(output_size), _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride)\n\u001b[0;32m   4693\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 2D or 3D (batch mode) tensor for input with possibly 0 batch size and non-zero dimensions for input, but got: [1, 16384, 0]"
     ]
    }
   ],
   "source": [
    "from notebook_helpers import run\n",
    "import os\n",
    "custom_steps = 200\n",
    "cond_choice_path = os.path.join(dir, cond_choice.value)\n",
    "logs = run(model[\"model\"], cond_choice_path, mode.value, custom_steps, up_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import IPython.display as d\n",
    "from PIL import Image\n",
    "\n",
    "sample = logs[\"sample\"]\n",
    "sample = sample.detach().cpu()\n",
    "sample = torch.clamp(sample, -1., 1.)\n",
    "sample = (sample + 1.) / 2. * 255\n",
    "sample = sample.numpy().astype(np.uint8)\n",
    "sample = np.transpose(sample, (0, 2, 3, 1))\n",
    "print(sample.shape)\n",
    "a = Image.fromarray(sample[0])\n",
    "display(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b11eaf0e94fc111c724b4ee42f36a114f13a982891d5c8efe533c6716daa5c0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
